# NABIP AMS - Next Steps Guide

## üéâ Completed Work Summary

### Database Schema Complete ‚úÖ

The complete NABIP AMS database schema has been established in Supabase with the following accomplishments:

#### **35 Tables Created**
- ‚úÖ 5 Discovery System tables (flexible web scraping data ingestion)
- ‚úÖ 6 Core Member & Chapter Management tables
- ‚úÖ 8 Events & Registration Management tables
- ‚úÖ 4 Financial Management tables
- ‚úÖ 2 Learning Management tables
- ‚úÖ 2 Communications tables
- ‚úÖ 2 System & Audit tables
- ‚úÖ 6 Supporting tables (credentials, licenses, practice areas, designations, leaders, news)

#### **34 Database Migrations Applied**
- ‚úÖ Migrations 001-005: Discovery system with schema fingerprinting
- ‚úÖ Migration 006: Discovery triggers and auto-detection functions
- ‚úÖ Migrations 007-012: Core member and chapter management
- ‚úÖ Migrations 013-020: Events and registration workflow
- ‚úÖ Migrations 021-024: Financial tracking and invoicing
- ‚úÖ Migrations 025-026: Course catalog and enrollments
- ‚úÖ Migrations 027-028: Email campaigns and templates
- ‚úÖ Migrations 029-030: Reports and audit logging
- ‚úÖ Migration 031: Deferred foreign key constraints
- ‚úÖ Migrations 032-033: Row Level Security (RLS) policies
- ‚úÖ Migration 034: Helper functions and auto-update triggers

#### **Key Features Implemented**

**Discovery-First Architecture:**
- MD5 schema fingerprinting for automatic change detection
- Field path discovery and mapping
- Transformation rules engine
- Schema evolution alerts

**Auto-Generated Identifiers:**
- UUIDs for all primary keys
- Sequential invoice numbering (`INV-YYYY-######`)
- QR codes for registrations (`REG-XXXXXXXXXXXX`)
- Confirmation numbers (`CONF-XXXXXXXXXX`)
- Certificate numbers (`CERT-YYYY-XXXXXXXX`)
- URL-safe slugs for courses and news

**Automatic Count Maintenance:**
- `chapters.member_count` ‚Üê Auto-updates when members join/leave
- `events.registered_count` ‚Üê Auto-updates with registrations
- `courses.enrolled_count` ‚Üê Auto-updates with enrollments
- `invoices.subtotal` & `invoices.total_amount` ‚Üê Auto-calculated from line items
- `campaign_templates.usage_count` ‚Üê Tracks template usage

**Auto-Calculated Metrics:**
- Campaign open rates, click rates, click-to-open rates
- Member engagement scores (0-100 scale)
- Invoice totals with tax calculations

**Row Level Security:**
- Enabled on all 30 core tables
- Helper functions using Supabase `auth.uid()`
- Policies for public, members, chapter leaders, and admins

#### **Documentation Created**

1. **DATABASE_SCHEMA.md** (410 lines)
   - Complete schema documentation
   - All 35 tables with purposes and key features
   - Design patterns and relationships
   - Sample queries for analytics
   - Maintenance tasks
   - Migration history

2. **DATA_IMPORT_GUIDE.md** (Comprehensive import guide)
   - Step-by-step import process
   - Discovery system usage
   - Transformation rule creation
   - Direct import examples
   - Sample test data
   - Validation queries
   - Troubleshooting section

3. **sample-data.sql** (Ready-to-execute script)
   - 6 chapters across hierarchical structure
   - 6 sample members with varying statuses
   - License, practice area, and designation data
   - 4 events (in-person, virtual, hybrid)
   - Ticket types and pricing tiers
   - 4 courses with enrollments
   - Registrations and chapter leaders
   - Validation queries to verify data integrity

4. **test-discovery-system.sql** (Discovery testing script)
   - Tests schema fingerprinting with varying field names
   - Simulates real web scraping scenarios
   - Validates auto-discovery mechanisms
   - Tests transformation rules creation
   - Demonstrates flexible data mapping

5. **validate-triggers.sql** (Comprehensive trigger tests)
   - Tests all 8 major trigger systems
   - Validates auto-count updates
   - Tests auto-generated identifiers
   - Verifies timestamp auto-updates
   - Tests invoice total calculations
   - Tests campaign metrics calculations
   - Production data health checks

---

## üöÄ Next Steps

### Phase 1: Import Your Web-Scraped Data

You mentioned you're currently web scraping the data. Here's how to import it:

#### **Option A: Use Discovery System (Recommended for First Import)**

```sql
-- 1. Register your scraping source
INSERT INTO scraped_data_sources (source_name, source_type, source_url, status)
VALUES ('NABIP Member Directory', 'member_directory', 'https://nabip.org/members', 'active')
RETURNING id;

-- 2. Insert your scraped JSON data
INSERT INTO scraped_data_raw (source_id, source_reference, parsed_data)
VALUES
  ('YOUR_SOURCE_ID_FROM_STEP_1', 'page-1', 'YOUR_JSON_DATA'::jsonb),
  ('YOUR_SOURCE_ID_FROM_STEP_1', 'page-2', 'YOUR_JSON_DATA'::jsonb);
  -- ... add all your scraped data

-- 3. Review discovered schemas
SELECT * FROM discovered_schemas
WHERE source_id = 'YOUR_SOURCE_ID_FROM_STEP_1'
ORDER BY occurrence_count DESC;

-- 4. Create transformation rules (see DATA_IMPORT_GUIDE.md)

-- 5. Execute direct import (see DATA_IMPORT_GUIDE.md)
```

#### **Option B: Direct Import (If you know your schema)**

```sql
-- Import chapters first
INSERT INTO chapters (name, chapter_type, status)
SELECT DISTINCT
  scraped->>'chapterName',
  'state',
  'active'
FROM scraped_data_raw
ON CONFLICT (name) DO NOTHING;

-- Import members
INSERT INTO members (email, first_name, last_name, chapter_id, status)
SELECT
  scraped->>'email',
  scraped->>'firstName',
  scraped->>'lastName',
  (SELECT id FROM chapters WHERE name = scraped->>'chapterName'),
  'active'::member_status_enum
FROM scraped_data_raw
ON CONFLICT (email) DO UPDATE SET
  first_name = EXCLUDED.first_name,
  last_name = EXCLUDED.last_name;
```

### Phase 2: Test the Schema

Execute the provided SQL scripts in your Supabase SQL Editor:

1. **Run sample-data.sql** to populate test data
   - Creates realistic sample members, chapters, events, courses
   - Tests all relationships and constraints
   - Run validation queries at the end

2. **Run test-discovery-system.sql** to validate discovery features
   - Tests schema fingerprinting
   - Validates auto-discovery
   - Tests transformation rules

3. **Run validate-triggers.sql** to verify all triggers
   - Tests all 8 auto-update mechanisms
   - Validates auto-calculated fields
   - Ensures data integrity

### Phase 3: Configure Supabase Authentication

#### **Set Up Auth Providers**

1. Navigate to Supabase Dashboard ‚Üí Authentication ‚Üí Providers
2. Enable providers:
   - Email/Password (for members without SSO)
   - Google OAuth (optional for easier login)
   - Magic Link (passwordless email login)

#### **Map Auth Users to Members Table**

Create a trigger that auto-creates member records when users sign up:

```sql
CREATE OR REPLACE FUNCTION handle_new_user()
RETURNS TRIGGER AS $
BEGIN
  INSERT INTO members (id, email, status)
  VALUES (NEW.id, NEW.email, 'pending')
  ON CONFLICT (email) DO UPDATE
  SET id = NEW.id;
  RETURN NEW;
END;
$ LANGUAGE plpgsql SECURITY DEFINER;

CREATE TRIGGER on_auth_user_created
  AFTER INSERT ON auth.users
  FOR EACH ROW EXECUTE FUNCTION handle_new_user();
```

#### **Test RLS Policies**

1. Sign in as a test member via Supabase Auth
2. Run queries to verify RLS is working:
   ```sql
   -- Should only return current user's data
   SELECT * FROM members WHERE id = auth.uid();

   -- Should return public events
   SELECT * FROM events WHERE visibility = 'public';
   ```

### Phase 4: Migrate UI from GitHub Spark to Supabase

#### **Replace useKV with Supabase Queries**

**Before (GitHub Spark):**
```typescript
import { useKV } from '@github-spark/utils';

function Members() {
  const [members, setMembers] = useKV('members', []);
  // ...
}
```

**After (Supabase):**
```typescript
import { createClient } from '@supabase/supabase-js';
import { useEffect, useState } from 'react';

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
);

function Members() {
  const [members, setMembers] = useState([]);

  useEffect(() => {
    async function fetchMembers() {
      const { data, error } = await supabase
        .from('members')
        .select('*')
        .eq('status', 'active');

      if (!error) setMembers(data);
    }
    fetchMembers();
  }, []);
  // ...
}
```

#### **Key Migration Tasks**

| File/Component | Current (Spark) | New (Supabase) | Priority |
|----------------|-----------------|----------------|----------|
| `src/lib/db.ts` | Create file | Supabase client config | Critical |
| Members List | `useKV('members')` | `supabase.from('members').select()` | High |
| Events List | `useKV('events')` | `supabase.from('events').select()` | High |
| Courses List | `useKV('courses')` | `supabase.from('courses').select()` | High |
| Add Member Form | `setKV('members')` | `supabase.from('members').insert()` | Critical |
| Registration | `useKV('registrations')` | `supabase.from('registrations').insert()` | High |
| Dashboard Analytics | Computed from KV | Server-side queries with aggregations | Medium |

#### **Install Supabase Dependencies**

```bash
npm install @supabase/supabase-js
npm install @supabase/ssr # For Next.js App Router
```

#### **Environment Variables**

Create `.env.local`:
```bash
NEXT_PUBLIC_SUPABASE_URL=your-project-url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key # Server-only
```

### Phase 5: Deploy to Independent Hosting

Once the UI is migrated from GitHub Spark, deploy to a platform like:

- **Vercel** (Recommended for Next.js)
  - Automatic deployments from GitHub
  - Edge functions support
  - Free tier available

- **Netlify** (Alternative)
  - Similar features to Vercel
  - Good for static sites

- **Railway** (For full-stack apps)
  - PostgreSQL included
  - Deploy from GitHub

#### **Deployment Checklist**

- [ ] All environment variables configured
- [ ] Supabase connection tested
- [ ] RLS policies validated
- [ ] Authentication flow working
- [ ] Database migrations applied
- [ ] Sample data loaded (or production data imported)
- [ ] Error handling implemented
- [ ] Loading states added
- [ ] 404/error pages created

---

## üìä Current Database Statistics

Based on the sample data provided:

- **6 Chapters** (1 national, 2 state, 3 local)
- **6 Members** (4 active, 1 pending, test accounts ready)
- **12+ Licenses** across CA, TX, FL, IL, WA states
- **4 Events** (conference, webinar, leadership summit, workshop)
- **4 Courses** (REBC designation, certifications, CE credits)
- **5 Enrollments** (various completion statuses)
- **5 Event Registrations** (confirmed and waitlisted)
- **4 Chapter Leaders** (presidents, VP, treasurer)

---

## üîß Maintenance & Monitoring

### Regular Tasks

**Monthly:**
```sql
-- Update member engagement scores
UPDATE members
SET engagement_score = calculate_engagement_score(id)
WHERE status = 'active';
```

**Quarterly:**
```sql
-- Archive processed scraped data
DELETE FROM scraped_data_raw
WHERE processing_status = 'processed'
  AND created_at < NOW() - INTERVAL '90 days';
```

**Annually:**
```sql
-- Archive old audit logs
UPDATE audit_logs
SET archived = true
WHERE created_at < NOW() - INTERVAL '7 years';
```

### Performance Monitoring

```sql
-- Check table sizes
SELECT
  tablename,
  pg_size_pretty(pg_total_relation_size('public.' || tablename)) as size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size('public.' || tablename) DESC;

-- Find slow reports
SELECT name, avg_execution_time_ms, execution_count
FROM reports
WHERE avg_execution_time_ms > 5000
ORDER BY avg_execution_time_ms DESC;
```

---

## üìö Key Documentation Files

All documentation is ready in your project:

1. **DATABASE_SCHEMA.md** - Complete database reference
2. **DATA_IMPORT_GUIDE.md** - Import your web-scraped data
3. **sample-data.sql** - Test data script
4. **test-discovery-system.sql** - Discovery system validation
5. **validate-triggers.sql** - Trigger verification
6. **README.md** - Project overview and features

---

## üéØ Recommended Timeline

| Phase | Estimated Time | Priority |
|-------|---------------|----------|
| **Phase 1: Import Web-Scraped Data** | 2-4 hours | Critical |
| **Phase 2: Test Schema** | 1-2 hours | High |
| **Phase 3: Configure Auth** | 2-3 hours | High |
| **Phase 4: Migrate UI** | 8-12 hours | Critical |
| **Phase 5: Deploy** | 2-4 hours | Medium |

**Total Estimated Time:** 15-25 hours

---

## ‚ö†Ô∏è Important Notes

1. **Data Import First**: Before migrating the UI, import your web-scraped data using the discovery system
2. **Test RLS Thoroughly**: Ensure Row Level Security policies work correctly before production
3. **Backup Strategy**: Set up automated backups in Supabase (included in paid tier)
4. **Monitor Performance**: Use the provided performance queries to track database health
5. **Migration Strategy**: Consider a phased migration from Spark to Supabase (run both temporarily)

---

## üÜò Support & Resources

- **Supabase Documentation**: https://supabase.com/docs
- **PostgreSQL JSONB**: https://www.postgresql.org/docs/current/datatype-json.html
- **RLS Best Practices**: https://supabase.com/docs/guides/auth/row-level-security
- **Next.js + Supabase**: https://supabase.com/docs/guides/getting-started/quickstarts/nextjs

---

## ‚úÖ Success Criteria

You'll know the migration is complete when:

- [ ] All web-scraped data is imported and validated
- [ ] Sample data tests pass (validate-triggers.sql shows all ‚úì PASS)
- [ ] Users can sign in via Supabase Auth
- [ ] RLS policies protect sensitive data
- [ ] UI components fetch from Supabase instead of useKV
- [ ] Create operations work (Add Member, Register for Event, etc.)
- [ ] Triggers auto-update counts correctly
- [ ] Application is deployed independently (not on GitHub Spark)

---

**The database foundation is ready. Time to import your data and build the future of NABIP AMS! üöÄ**
